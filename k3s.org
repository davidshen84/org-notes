#+TITLE: k3s notes
#+AUTHOR: David S.
#+DATE: <2023-08-27 Sun>
#+STARTUP: showall hideblocks

* readme üôè
- cofigure your ssh and make sure it is secure
- some of the code blocks are executed with *root privilege*
- some of the babel blocks are tangled with *root privilege*
- some of the ~kubectl apply~ commands are protected by the ~--dry-run~ option
- update the *absolute paths* according to your envirnment

* installation
https://docs.k3s.io/

** server

If you are in a virtual envirnment, like WSL, the cluster should
advise the IP address of the hosting environment to the agents. Set
the ~node-external-ip~ to the hosting machine's IP address.

*** wsl kernel features
WSL 2 instances sit behind the host network and require NAT on the
host to access it. But k8s requires all pods can communicate with each
other without NAT. So we need a CNI that can bring the WSL instance
and the other hosts into the same network segment.

There should be many ways to achieve this. In this notebook, I used
the ~wireguard~ method provided by the `flannel` CNI. It basically
creates a VPN and connects all the pods in the VPN.

To use ~wireguard-native~, all the nodes must be able to connect
through a wireguard VPN. This requires the kernels of *all* the nodes
have the nesessary kernel features enabled.

#+begin_example
  CONFIG_NETFILTER_XT_CONNMARK=y
  CONFIG_NETFILTER_XT_MATCH_CONNMARK=y
  CONFIG_NETFILTER_XT_TARGET_CONNMARK=y
#+end_example

** install k3s on server node
#+name: server-config
#+begin_src yaml :mkdirp yes :tangle /sudo::/etc/rancher/k3s/config.yaml :comments link
  write-kubeconfig-mode: "0644"
  disable:
    - traefik
  node-external-ip: 192.168.86.109
  advertise-address: 192.168.86.109
  token: k3s-home
  tls-san:
    - 192.168.86.109
  flannel-backend: wireguard-native
  flannel-external-ip: true
#+end_src

#+name: registry
#+begin_src yaml :mkdir yes :tangle /sudo::/etc/rancher/k3s/registries.yaml :comments link
  mirrors:
    registry.default:
      endpoint:
        - "http://registry.default:5000"
#+end_src

#+begin_src shell :dir /sudo::/root :results output
  curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server" sh -s -
#+end_src

#+begin_src shell :results output :wrap src yaml
  yq '.contexts[0].name = "k3s"' /etc/rancher/k3s/k3s.yaml | tee ~/.kube/k3s.yaml
#+end_src

** install k3s on agent node
#+name: agent-config
#+begin_src yaml :mkdirp yes :tangle /ssh:gentoo|sudo:gentoo:/etc/rancher/k3s/config.yaml :comments link
  token: k3s-home
  server: https://192.168.86.109:6443
#+end_src

#+begin_src shell :dir /ssh:gentoo|sudo:gentoo:~/ :results verbatim
  curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="agent" sh -s -
#+end_src

* nvidia runtime
https://docs.k3s.io/advanced#nvidia-container-runtime-support

Only install this component on the *server* side.

#+begin_src yaml :tangle /sudo::/var/lib/rancher/k3s/server/manifests/nvidia-runtime-class.yaml
  apiVersion: node.k8s.io/v1
  kind: RuntimeClass
  metadata:
    name: nvidia
  handler: nvidia
#+end_src

** k8s-device-plugin
https://github.com/NVIDIA/k8s-device-plugin/

You need to install [[https://github.com/NVIDIA/k8s-device-plugin/#install-the-nvidia-container-toolkit][the toolkit]] first.

Install ~helm~ and add these *helm-repo*.
#+begin_src shell :results output
  helm repo add nvdp https://nvidia.github.io/k8s-device-plugin
  helm repo add nvgfd https://nvidia.github.io/gpu-feature-discovery
  helm repo update
#+end_src

#+begin_src shell :results output :wrap src yaml
  helm show values nvdp/nvidia-device-plugin
#+end_src

#+name: nvidia-device-plugin-values
#+begin_src yaml
  runtimeClassName: nvidia

  image:
    repository: registry.gitlab.com/nvidia/kubernetes/device-plugin/staging/k8s-device-plugin
    tag: "8b416016"
#+end_src

#+begin_src shell :noweb yes :results output
  helm upgrade --install nvdp \
       nvdp/nvidia-device-plugin --version 0.14.1 \
       --namespace nvidia-device-plugin --create-namespace \
       --values - --dry-run <<EOF
  <<nvidia-device-plugin-values>>
  EOF
#+end_src

#+begin_src shell :results output :wrap src yaml
  helm show values nvdp/nvidia-device-plugin --version 0.14.1
#+end_src

** gpu-feature-discovery
https://github.com/NVIDIA/gpu-feature-discovery/

#+begin_src shell
  helm search repo nvgfd
#+end_src

#+begin_src shell :results output :wrap src yaml
  helm show values nvgfd/gpu-feature-discovery --version 0.8.1
#+end_src

#+name: gpu-feature-discovery-values
#+begin_src yaml
  nodeSelector:
    kubernetes.io/hostname: desktop-0xivd7b-wsl

  runtimeClassName: nvidia
#+end_src

#+begin_src shell :noweb yes :results output
  helm upgrade --install nvgfd \
       nvgfd/gpu-feature-discovery --version 0.8.1 \
       --namespace gpu-feature-discovery --create-namespace --values - <<EOF
  <<gpu-feature-discovery-values>>
  EOF
#+end_src

* tests

#+name: gpu-test
#+begin_src yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: nbody-gpu-benchmark
    namespace: default
  spec:
    restartPolicy: OnFailure
    runtimeClassName: nvidia
    nodeSelector:
      kubernetes.io/hostname: desktop-0xivd7b-wsl
    containers:
    - name: cuda-container
      image: nvcr.io/nvidia/k8s/cuda-sample:nbody
      args: ["nbody", "-gpu", "-benchmark"]
      resources:
        limits:
          nvidia.com/gpu: 1
      env:
      - name: NVIDIA_VISIBLE_DEVICES
        value: all
      - name: NVIDIA_DRIVER_CAPABILITIES
        value: all
#+end_src

#+begin_src shell :noweb yes results: output
  cat<<EOF | kubectl apply -f - --dry-run=client
  <<gpu-test>>
  EOF
#+end_src

#+name: dnsutils
#+begin_src yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: dnsutils
    namespace: default
  spec:
    nodeSelector:
      kubernetes.io/hostname: gentoo
    containers:
    - name: dnsutils
      image: registry.k8s.io/e2e-test-images/jessie-dnsutils:1.3
      command:
        - sleep
        - "infinity"
      imagePullPolicy: IfNotPresent
    restartPolicy: Always
#+end_src

#+begin_src shell :noweb yes
  cat<<EOF | kubectl apply --namespace default --filename -
  <<dnsutils>>
  EOF

#+end_src

#+name: busybox
#+begin_src yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: busybox-pod
  spec:
    containers:
    - name: busybox
      image: busybox
      command: ["sleep", "infinity"]
      volumeMounts:
      - mountPath: /notebooks
        name: notebooks
    volumes:
    - name: notebooks
      hostPath:
        path: /home/david/notebooks
        type: Directory
#+end_src

#+begin_src shell :noweb yes :results output
  kubectl apply -n default -f-<<EOF
  <<busybox>>
  EOF

#+end_src
